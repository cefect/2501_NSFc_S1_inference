# inference submit file


#set the image
universe = container
container_image = docker://cefect/pytorch-2.6.0-cuda12.4_terratorch:v0
 

# Your job's launcher script (transferred and executed inside the container)
executable      = htcondor/run_test.sh

# If main.py needs args, add them here; they reach run.sh as "$@"
#arguments  = data/array.txt result.$(Cluster).$(Process).txt

# --- file transfer (required for Docker/containers) ---
should_transfer_files   = YES
when_to_transfer_output = ON_EXIT
preserve_relative_paths = True

# Ship your code + the small input from /home
transfer_input_files = src

# Explicitly pull back the program output (top-level file)
#transfer_output_files = result.$(Cluster).$(Process).txt
output_destination = /home/sbryant8/LS/10_IO/2501_NSFc/s1_infer/

# --- logs on the SUBMIT node (your /home) ---
output = logs/$(Cluster).$(Process).$(SUBMIT_TIME).out
error  = logs/$(Cluster).$(Process).$(SUBMIT_TIME).err
log    = logs/$(Cluster).$(SUBMIT_TIME).log

# Resources (tune as needed)
request_cpus   = 1
request_memory = 4GB
request_disk   = 4GB

# GPU
request_gpus = 1
#gpus_minimum_capability = 7.5
gpus_minimum_runtime = 12.4
#mininum quantity of GPU memory in MiB
gpus_minimum_memory = 4096 
+WantGPULab = true
+GPUJobLength = "short"

 
queue
